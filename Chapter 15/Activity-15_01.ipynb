{"cells":[{"cell_type":"markdown","source":["## **Activity 15.01**\n","### Fitting a Logistic Regression Model"],"metadata":{"id":"Jbs4PQh8Ocnd"}},{"cell_type":"markdown","metadata":{"id":"w5zf0mqcQMGt"},"source":["### Importing Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWzRC58yLrPu"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"MKGcsUArQPi6"},"source":["### Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vugw1EMtQXIN"},"outputs":[],"source":["#Loading data from the Github repository to colab notebook\n","filename = 'https://raw.githubusercontent.com/fenago/DSBook/main/Chapter%2015/australian1.csv'"]},{"cell_type":"markdown","metadata":{"id":"6lhVDuCCQa0y"},"source":["### Reading File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"an7WjUqVL2mj"},"outputs":[],"source":["# Loading the data using pandas\n","\n","credData = pd.read_csv(filename,sep=\",\",header = None,na_values = \"?\")\n","credData.head()"]},{"cell_type":"markdown","metadata":{"id":"MysNNAQHQYSC"},"source":["### Changing Classes to 1 and 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkySkZ37L3ex"},"outputs":[],"source":["# Changing the Classes to 1 & 0\n","credData.loc[credData[16] == '+' , 16] = 1\n","credData.loc[credData[16] == '-' , 16] = 0\n","credData.head()"]},{"cell_type":"markdown","metadata":{"id":"a9Cx9TFcRA6D"},"source":["### Removing the rows with na values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSnCnHmmL7-B"},"outputs":[],"source":["# Dropping all the rows with na values\n","newcred = credData.dropna(axis = 0)\n","newcred.shape"]},{"cell_type":"markdown","metadata":{"id":"hWp2aZaeRT1u"},"source":["### Making dummy values from the categorical variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyW_pktLL-yI"},"outputs":[],"source":["# Seperating the categorical variables to make dummy variables\n","\n","credCat = pd.get_dummies(newcred[[2,5,9,10]])"]},{"cell_type":"markdown","metadata":{"id":"PQjzeTfDRZNJ"},"source":["### Separating the numberical variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bq5rijcPP-Hs"},"outputs":[],"source":["# Seperating the numerical variables\n","credNum = newcred[[0,1,3,4,6,7,8,11,12,13,14,15]]"]},{"cell_type":"markdown","metadata":{"id":"C1gDhRuURdfH"},"source":["### Creating X and Y variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zOZeJJFP-hW"},"outputs":[],"source":["# Making the X variable which is a concatenation of categorical and numerical data\n","\n","X = pd.concat([credCat,credNum],axis = 1)\n","print(X.shape)\n","\n","# Seperating the label as y variable\n","y = pd.Series(newcred[16], dtype=\"int\")\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"-xYaxpmuRikm"},"source":["### Normalizing the dataset using MinMaxSacler() function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjgJ-yEQQACc"},"outputs":[],"source":["# Normalising the data sets\n","# Import library function\n","from sklearn import preprocessing\n","# Creating the scaling function\n","minmaxScaler = preprocessing.MinMaxScaler()\n","# Transforming with the scaler function\n","X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))"]},{"cell_type":"markdown","metadata":{"id":"TDHqAG5TRo-6"},"source":["### Splitting the dataset into training and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPKQdd_4QBTg"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Splitting the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_tran, y, test_size=0.3, random_state=123)"]},{"cell_type":"markdown","source":["### Fitting the model"],"metadata":{"id":"UYJxWxVL5Kqj"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","# Defining the LogisticRegression function\n","benchmarkModel = LogisticRegression()\n","# Fitting the model\n","benchmarkModel.fit(X_train, y_train)"],"metadata":{"id":"JNqb2_Hb5B6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Finding prediction and accuracy metrics"],"metadata":{"id":"tfw3IFsj6CyX"}},{"cell_type":"code","source":["# Prediction and accuracy metrics\n","pred = benchmarkModel.predict(X_test)\n","print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(benchmarkModel.score(X_test, y_test)))"],"metadata":{"id":"A_X0XT5d5DYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating Confusion matrix and Classification report"],"metadata":{"id":"x52zd6NG5qA-"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","# Confusion Matrix for the model\n","print(confusion_matrix(y_test, pred))\n","# Classification report for the model\n","print(classification_report(y_test, pred))"],"metadata":{"id":"IlEFI-hf5Ivl"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Activity-15_01.ipynb","provenance":[{"file_id":"1mHsPex7KHEfAZfkvDfpmk6eEhodLlqkr","timestamp":1650473216592}],"collapsed_sections":["w5zf0mqcQMGt","MKGcsUArQPi6","6lhVDuCCQa0y","MysNNAQHQYSC","a9Cx9TFcRA6D","hWp2aZaeRT1u","PQjzeTfDRZNJ","C1gDhRuURdfH","-xYaxpmuRikm","TDHqAG5TRo-6"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}