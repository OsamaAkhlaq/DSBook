{"cells":[{"cell_type":"markdown","source":["## **Exercise 15.03**\n","### Ensemble Model Using the Weighted Averaging Technique"],"metadata":{"id":"UuSODHgWPcsH"}},{"cell_type":"markdown","metadata":{"id":"w5zf0mqcQMGt"},"source":["### Importing Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWzRC58yLrPu"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"MKGcsUArQPi6"},"source":["### Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vugw1EMtQXIN"},"outputs":[],"source":["#Loading data from the Github repository to colab notebook\n","filename = 'https://raw.githubusercontent.com/fenago/DSBook/main/Chapter%2015/australian1.csv'"]},{"cell_type":"markdown","metadata":{"id":"6lhVDuCCQa0y"},"source":["### Reading File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"an7WjUqVL2mj"},"outputs":[],"source":["# Loading the data using pandas\n","\n","credData = pd.read_csv(filename,sep=\",\",header = None,na_values = \"?\")\n","credData.head()"]},{"cell_type":"markdown","metadata":{"id":"MysNNAQHQYSC"},"source":["### Changing Classes to 1 and 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkySkZ37L3ex"},"outputs":[],"source":["# Changing the Classes to 1 & 0\n","credData.loc[credData[16] == '+' , 16] = 1\n","credData.loc[credData[16] == '-' , 16] = 0\n","credData.head()"]},{"cell_type":"markdown","metadata":{"id":"a9Cx9TFcRA6D"},"source":["### Removing the rows with na values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSnCnHmmL7-B"},"outputs":[],"source":["# Dropping all the rows with na values\n","newcred = credData.dropna(axis = 0)\n","newcred.shape"]},{"cell_type":"markdown","metadata":{"id":"hWp2aZaeRT1u"},"source":["### Making dummy values from the categorical variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyW_pktLL-yI"},"outputs":[],"source":["# Seperating the categorical variables to make dummy variables\n","\n","credCat = pd.get_dummies(newcred[[2,5,9,10]])"]},{"cell_type":"markdown","metadata":{"id":"PQjzeTfDRZNJ"},"source":["### Separating the numerical variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bq5rijcPP-Hs"},"outputs":[],"source":["# Seperating the numerical variables\n","\n","credNum = newcred[[0,1,3,4,6,7,8,11,12,13,14,15]]"]},{"cell_type":"markdown","metadata":{"id":"C1gDhRuURdfH"},"source":["### Creating X and Y variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zOZeJJFP-hW"},"outputs":[],"source":["# Making the X variable which is a concatenation of categorical and numerical data\n","\n","X = pd.concat([credCat,credNum],axis = 1)\n","print(X.shape)\n","\n","# Seperating the label as y variable\n","y = pd.Series(newcred[16], dtype=\"int\")\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"-xYaxpmuRikm"},"source":["### Normalizing the dataset using MinMaxSacler() function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjgJ-yEQQACc"},"outputs":[],"source":["# Normalising the data sets\n","# Import library function\n","from sklearn import preprocessing\n","# Creating the scaling function\n","minmaxScaler = preprocessing.MinMaxScaler()\n","# Transforming with the scaler function\n","X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))"]},{"cell_type":"markdown","metadata":{"id":"TDHqAG5TRo-6"},"source":["### Splitting the dataset into training and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPKQdd_4QBTg"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Splitting the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_tran, y, test_size=0.3, random_state=123)"]},{"cell_type":"markdown","source":["### Importing 3 classifiers to be used as base models"],"metadata":{"id":"yBfOkiS-STzq"}},{"cell_type":"code","source":["# Defining three base models\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","\n","model1 = LogisticRegression(random_state=123)\n","model2 = KNeighborsClassifier(n_neighbors=5)\n","model3 = RandomForestClassifier(n_estimators=500)"],"metadata":{"id":"QZCfqLG5SRo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fitting all 3 models on the Training set"],"metadata":{"id":"SBpr0Pf6S1VC"}},{"cell_type":"code","source":["# Fitting all three models on the training data\n","model1.fit(X_train,y_train)\n","model2.fit(X_train,y_train)\n","model3.fit(X_train,y_train)"],"metadata":{"id":"NRXzV70dSiOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicting the probabilities of each model using  predict_proba() function"],"metadata":{"id":"HV1J0vi6S9ZH"}},{"cell_type":"code","source":["# Predicting probabilities of each model on the test set\n","pred1=model1.predict_proba(X_test)\n","pred2=model2.predict_proba(X_test)\n","pred3=model3.predict_proba(X_test)"],"metadata":{"id":"xEl-ngmQSlOj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Iteration1: For Weights"],"metadata":{"id":"zeSZu9I5Ug2N"}},{"cell_type":"markdown","source":["### Averaging the prediction generated from all three models"],"metadata":{"id":"VMzqUuB5TF5o"}},{"cell_type":"code","source":["# Calculating the ensemble prediction by applying weights for each prediction\n","ensemblepred=(pred1 *0.60+pred2 * 0.20+pred3 * 0.20)"],"metadata":{"id":"eKT02IOgSnq3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Displaying first 4 rows of the ensemble prediction array"],"metadata":{"id":"O_rBltjhTKzF"}},{"cell_type":"code","source":["# Displaying first 4 rows of the ensemble predictions\n","ensemblepred[0:4,:]"],"metadata":{"id":"EIhFUwt2Sn2t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Printing the order of each class from the prediction output"],"metadata":{"id":"Hzx2-YgcTQJV"}},{"cell_type":"code","source":["# Printing the order of classes for each model\n","print(model1.classes_)\n","print(model2.classes_)\n","print(model3.classes_)"],"metadata":{"id":"7RC80UCQSqFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating predictiond from the prbabilities"],"metadata":{"id":"UXQy1jc6TXGd"}},{"cell_type":"code","source":["# Generating predictions from probabilities\n","import numpy as np\n","pred = np.argmax(ensemblepred,axis = 1)\n","pred"],"metadata":{"id":"8zscD2DvSrdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating confusion matrix for the predictions"],"metadata":{"id":"dagZCU91TlY-"}},{"cell_type":"code","source":["# Generating confusion matrix\n","from sklearn.metrics import confusion_matrix\n","confusionMatrix = confusion_matrix(y_test, pred)\n","print(confusionMatrix)"],"metadata":{"id":"yZo9WojWSskA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating classification report"],"metadata":{"id":"Gq-sXgH9TpcC"}},{"cell_type":"code","source":["# Generating classification report\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, pred))"],"metadata":{"id":"XQjLrlFtSuXg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Iteration2"],"metadata":{"id":"VGL2xDaSVFHx"}},{"cell_type":"markdown","source":["### Averaging the prediction generated from all three models"],"metadata":{"id":"FHABfQz3VYns"}},{"cell_type":"code","source":["# Calculating the ensemble prediction by applying weights for each prediction\n","ensemblepred=(pred1 *0.70+pred2 * 0.15+pred3 * 0.15)"],"metadata":{"id":"FfB5cOR-Vaid"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating predictions from probabilities"],"metadata":{"id":"IseDW1zxVmtR"}},{"cell_type":"code","source":["# Generating predictions from probabilities\n","import numpy as np\n","pred = np.argmax(ensemblepred,axis = 1)"],"metadata":{"id":"Cjc_x9sqVb1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating confusion matrix"],"metadata":{"id":"ymOIwyR4Vp5O"}},{"cell_type":"code","source":["# Generating confusion matrix\n","from sklearn.metrics import confusion_matrix\n","confusionMatrix = confusion_matrix(y_test, pred)\n","print(confusionMatrix)"],"metadata":{"id":"-CbaBEIaVdPI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating classification report"],"metadata":{"id":"CvrKDWUcVsIE"}},{"cell_type":"code","source":["# Generating classification report\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, pred))"],"metadata":{"id":"pEKf1c9bVfB9"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Ex-15_03.ipynb","provenance":[{"file_id":"1rGae09acvzUsEuIYWN_Zd-3yz0doSq8q","timestamp":1650396776789},{"file_id":"1mHsPex7KHEfAZfkvDfpmk6eEhodLlqkr","timestamp":1650396072958}],"collapsed_sections":["w5zf0mqcQMGt","6lhVDuCCQa0y","MysNNAQHQYSC","a9Cx9TFcRA6D","hWp2aZaeRT1u","PQjzeTfDRZNJ","C1gDhRuURdfH","-xYaxpmuRikm","TDHqAG5TRo-6","yBfOkiS-STzq","SBpr0Pf6S1VC","HV1J0vi6S9ZH","VMzqUuB5TF5o","O_rBltjhTKzF","Hzx2-YgcTQJV","UXQy1jc6TXGd","dagZCU91TlY-","Gq-sXgH9TpcC","FHABfQz3VYns","IseDW1zxVmtR","ymOIwyR4Vp5O","CvrKDWUcVsIE"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}