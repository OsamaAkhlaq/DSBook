{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practice Exercise 14","provenance":[{"file_id":"1mG8gEn3OWKMbD9LdwthlkK_tr19KcmUw","timestamp":1650293226502},{"file_id":"1EG-8c-V2Pypl7NbrZzfKkF0h2yaTp6sZ","timestamp":1650291859382},{"file_id":"1yOG_PZqUy9-E885DzHYDwxQxtpTXWBVc","timestamp":1650212333509}],"collapsed_sections":["k5wiSFUICXNj","PqE9wfDvCoqC","f2sJ3xoXE-RN","Nb6_HuvOFpi3","AVL_N5IdF84H","h1ADw3WxGX_0","CMrW5QsVGdAr","rxjyIUubCWun","LbjDowJDCaTf","Ti794TypEYvS","V2FPjO-sEnVV","B2JWYtmwC5Us","fhu4LwzNFqyv","tUwNIUx5F6rl","Hed4pJ4KGPSL"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Practice Exercise 14**"],"metadata":{"id":"HH-rJDivR9B4"}},{"cell_type":"markdown","source":["### Importing Modules"],"metadata":{"id":"k5wiSFUICXNj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3ZLhsDjB4HU"},"outputs":[],"source":["import pandas as pd\n"]},{"cell_type":"markdown","source":["### Loading  Data"],"metadata":{"id":"PqE9wfDvCoqC"}},{"cell_type":"code","source":["# Defining file name of the git hub repository\n","\n","filename = './hcvdata.csv'\n"],"metadata":{"id":"HnjcyPM6CPz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the data using pandas\n","\n","adData = pd.read_csv(filename,sep=\",\",header = None,error_bad_lines=False)\n","adData.head()"],"metadata":{"id":"HR08AVBlCRH-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Seperating the dependent and independent variables from the dataset"],"metadata":{"id":"f2sJ3xoXE-RN"}},{"cell_type":"code","source":["# Seperating the dependent and independent variables\n","# Preparing the X variables\n","X = adData.loc[:,0:10]\n","print(X.shape)\n","# Preparing the Y variable\n","Y = adData[11]\n","print(Y.shape)"],"metadata":{"id":"tSbnf3p1Dsot"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Replacing special characters with NaN values for first 3 columns"],"metadata":{"id":"Nb6_HuvOFpi3"}},{"cell_type":"code","source":["# Replacing special characters in first 3 columns which are of type object\n","for i in range(0,3):\n","  X[i] = X[i].str.replace(\"?\", 'NaN').values.astype(float)\n","print(X.head(15))"],"metadata":{"id":"hU4RMgRhD7qB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Replacing special characters for Integer values"],"metadata":{"id":"AVL_N5IdF84H"}},{"cell_type":"code","source":["# Replacing special characters in the remaining columns which are of type integer\n","for i in range(3,11):\n","  X[i] = X[i].replace(\"?\", 'NaN').values.astype(float) "],"metadata":{"id":"dBrqtzvDD8Jp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Imputing Mean of each column for NaN values using .mean() function"],"metadata":{"id":"h1ADw3WxGX_0"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Imputing the 'NaN'  with mean of the values\n","for i in range(0,10):\n","  X[i] = X[i].fillna(X[i].mean())\n","  "],"metadata":{"id":"rAMZZIIPD-sc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalizing the dataset using minimaxScaler() function."],"metadata":{"id":"CMrW5QsVGdAr"}},{"cell_type":"code","source":["# Normalising the data sets\n","# Import library function\n","from sklearn import preprocessing\n","# Creating the scaling function\n","minmaxScaler = preprocessing.MinMaxScaler()\n","# Transforming with the scaler function\n","X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))\n","# Printing the output\n","X_tran.head()"],"metadata":{"id":"aLYuvyS8EBVw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating a high dimension dataset"],"metadata":{"id":"rxjyIUubCWun"}},{"cell_type":"code","source":["# Creating a high dimension data set\n","X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 2)))"],"metadata":{"id":"3V-yhEQdCUrr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Printing the dimension of the data set\n"],"metadata":{"id":"LbjDowJDCaTf"}},{"cell_type":"code","source":["# Printing the dimension of the data set\n","X_hd.shape"],"metadata":{"id":"QuU1uo0iCnlm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Adding noise to the dataset by defining mean and standard deviation and generating samples from distribution."],"metadata":{"id":"Ti794TypEYvS"}},{"cell_type":"code","source":["# Defining the mean and standard deviation\n","mu, sigma = 0, 0.1 "],"metadata":{"id":"W52U0599EV9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generating samples from the distribution\n","noise = np.random.normal(mu, sigma, [615,22]) \n","noise.shape"],"metadata":{"id":"yu2Z97aaEXS4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating a new dataset by adding noise"],"metadata":{"id":"V2FPjO-sEnVV"}},{"cell_type":"code","source":["# Creating a new data set by adding noise\n","X_new = X_hd + noise"],"metadata":{"id":"lAP4pHsgEl06"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Splitting the dataset into train and test sets"],"metadata":{"id":"B2JWYtmwC5Us"}},{"cell_type":"code","source":["# Splitting data set into train and test sets\n","from sklearn.model_selection import train_test_split\n","# Splitting the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.3, random_state=123)\n","\n","print('Training set shape',X_train.shape)\n","\n","print('Test set shape',X_test.shape)"],"metadata":{"id":"niyvvmvHCghf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Principal Component Analysis"],"metadata":{"id":"fhu4LwzNFqyv"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","import time\n","t0 = time.time()\n","pca = PCA(n_components=20)\n","# Fitting the PCA on the training set\n","pca.fit(X_train)\n","t1 = time.time()\n","print(\"PCA fitting time:\", round(t1-t0, 3), \"s\")"],"metadata":{"id":"N0JYkRhIFvz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transforming training set and test set\n","X_pca = pca.transform(X_train)\n","X_test_pca = pca.transform(X_test)\n","print(\"original shape of Training set:   \", X_train.shape)\n","print(\"original shape of Test set:   \", X_test.shape)\n","print(\"Transformed shape of training set:\", X_pca.shape)\n","print(\"Transformed shape of test set:\", X_test_pca.shape)"],"metadata":{"id":"DBmfAnZRFwIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","import time\n","\n","pcaModel = LogisticRegression()\n","\n","t0 = time.time()\n","pcaModel.fit(X_pca, y_train)\n","t1 = time.time()\n","\n","print(\"Total training time:\", round(t1-t0, 3), \"s\")"],"metadata":{"id":"Mk7Eo1OPFxzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predicting with the pca model\n","pred = pcaModel.predict(X_test_pca)\n","print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(pcaModel.score(X_test_pca, y_test)))"],"metadata":{"id":"0k5x-a4nF5Iz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generating confusion matrix\n","from sklearn.metrics import confusion_matrix\n","\n","confusionMatrix = confusion_matrix(y_test, pred)\n","print(confusionMatrix)"],"metadata":{"id":"acGQHOjEFzXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","# Getting the Classification_report\n","print(classification_report(y_test, pred))"],"metadata":{"id":"R5NFlBaWF1G4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Independent Component Analysis"],"metadata":{"id":"tUwNIUx5F6rl"}},{"cell_type":"code","source":["# Defining the ICA with number of components\n","from sklearn.decomposition import FastICA \n","ICA = FastICA(n_components=300, random_state=123) \n","# Fitting the ICA method and transforming the training set and noting the time\n","import time\n","t0 = time.time()\n","X_ica=ICA.fit_transform(X_train)\n","t1 = time.time()\n","print(\"ICA fitting time:\", round(t1-t0, 3), \"s\")"],"metadata":{"id":"Y1JFrqHfGBGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transfroming the test set \n","X_test_ica=ICA.transform(X_test)\n","print(\"original shape of Training set:   \", X_train.shape)\n","print(\"original shape of Test set:   \", X_test.shape)\n","print(\"Transformed shape of training set:\", X_ica.shape)\n","print(\"Transformed shape of test set:\", X_test_ica.shape)"],"metadata":{"id":"T34-CrLrGDQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","import time\n","\n","icaModel = LogisticRegression()\n","\n","t0 = time.time()\n","icaModel.fit(X_ica, y_train)\n","t1 = time.time()\n","\n","print(\"Total training time:\", round(t1-t0, 3), \"s\")"],"metadata":{"id":"8ZauhXl5GDt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predicting with the ica model\n","pred = icaModel.predict(X_test_ica)\n","print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(icaModel.score(X_test_ica, y_test)))"],"metadata":{"id":"8McuURX3GLA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generating confusion matrix\n","from sklearn.metrics import confusion_matrix\n","\n","confusionMatrix = confusion_matrix(y_test, pred)\n","print(confusionMatrix)"],"metadata":{"id":"6f4nn00dGLmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","# Getting the Classification_report\n","print(classification_report(y_test, pred))"],"metadata":{"id":"ufsSC1_5GN-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Factor Analysis"],"metadata":{"id":"Hed4pJ4KGPSL"}},{"cell_type":"code","source":["# Defining the number of factors\n","from sklearn.decomposition import FactorAnalysis\n","fa = FactorAnalysis(n_components = 30,random_state=123)\n","# Fitting the Factor analysis method and transforming the training set\n","import time\n","t0 = time.time()\n","X_fac=fa.fit_transform(X_train)\n","t1 = time.time()\n","print(\"Factor analysis fitting time:\", round(t1-t0, 3), \"s\")"],"metadata":{"id":"GTWXjH5JGRzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transfroming the test set \n","X_test_fac=fa.transform(X_test)\n","print(\"original shape of Training set:   \", X_train.shape)\n","print(\"original shape of Test set:   \", X_test.shape)\n","print(\"Transformed shape of training set:\", X_fac.shape)\n","print(\"Transformed shape of test set:\", X_test_fac.shape)"],"metadata":{"id":"hop_v8uxGTKX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","import time\n","\n","facModel = LogisticRegression()\n","\n","t0 = time.time()\n","facModel.fit(X_fac, y_train)\n","t1 = time.time()\n","\n","print(\"Total training time:\", round(t1-t0, 3), \"s\")"],"metadata":{"id":"mxjDaxawGUbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predicting with the factor analysis model\n","pred = facModel.predict(X_test_fac)\n","print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(facModel.score(X_test_fac, y_test)))"],"metadata":{"id":"0HfV8Q-FGbVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generating confusion matrix\n","from sklearn.metrics import confusion_matrix\n","\n","confusionMatrix = confusion_matrix(y_test, pred)\n","print(confusionMatrix)"],"metadata":{"id":"ZgLHvIsGGX6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","# Getting the Classification_report\n","print(classification_report(y_test, pred))"],"metadata":{"id":"JqQepNfJGYEr"},"execution_count":null,"outputs":[]}]}